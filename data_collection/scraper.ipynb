{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**15min.lt scraper**\n",
        "##Scraper will be divided in a few key areas and will be oriented to scrape the data from the news website 15min.lt. The main tool to scarpe the data from the website will be Newspaper3k\n",
        "##Table of contnet\n",
        "* 1. URL of news collection from 15min.lt\n",
        "* 2. Data scraping from URLs\n",
        "* 3. Data cleaning and preparation for fine-tuning"
      ],
      "metadata": {
        "id": "kkUneVx3-PuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URL of news collection from 15min.lt\n",
        "\n",
        "The following code block will save URLs from www.15min.lt and save them to CSV file. It usually saves between 900-1000 URLs, however while repeaded daily about the half will be dublicates, so the actual number of new URLs are about 400-500. Depending of the project that you are working on, some of the articles will be videos or less then 150 words, so not all of them will be suitable. In order to collect the data for project it could take a couple of weeks."
      ],
      "metadata": {
        "id": "kTh2pO3JAkO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import newspaper\n",
        "import pandas as pd\n",
        "\n",
        "news_site = newspaper.build(\"https://www.15min.lt/\", language=\"lt\")\n",
        "\n",
        "print(f\"Rasta straipsnių: {len(news_site.articles)}\")\n",
        "\n",
        "url_list = []\n",
        "\n",
        "for article in news_site.articles:\n",
        "    url_list.append(article.url)\n",
        "\n",
        "for url in url_list[:5]:\n",
        "    print(url)\n",
        "\n",
        "df_urls = pd.DataFrame(url_list, columns=[\"url\"])\n",
        "df_urls.to_csv(\"15min_straipsniu_url.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "h6WcSYmlAtcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data scraping from URLs"
      ],
      "metadata": {
        "id": "z8mND_wAAtaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages\n",
        "\n",
        "* newspaper3k - for articles crawling from URLs\n",
        "* lxml[html_clean] - to help clean HTML (adverts, tags or other unwanted elements)\n",
        "* lt_core_news_md - medium size Lithuanian spaCy model"
      ],
      "metadata": {
        "id": "9i0lLzkQClp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install newspaper3k\n",
        "!pip install lxml[html_clean]\n",
        "!python -m spacy download lt_core_news_md"
      ],
      "metadata": {
        "id": "RbeWzKoOAtYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import modules and functions\n",
        "* urlparse - used to analize URL structure to extract categories and subcategories from URLs (newspaper3k had trouble extracting subcategories and were mixing them up with caegories)\n",
        "* Article - used to extract article data from the URL\n",
        "* re - used for HTML cleaning\n",
        "* requests - for scanning web page content\n",
        "* nltk - needed for newspaper3k nlp() method for sentence division\n",
        "* spacy - used for text lemmatization\n"
      ],
      "metadata": {
        "id": "25unMw7tAtPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from spacy.lang.lt.examples import sentences\n",
        "spacy_model = spacy.load(\"lt_core_news_md\")"
      ],
      "metadata": {
        "id": "CUVRjWYfRFGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop words\n"
      ],
      "metadata": {
        "id": "pal9AvxrRPA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /usr/local/lib/python3.11/dist-packages/newspaper/resources/text/\n",
        "!cp stopwords-lt.txt /usr/local/lib/python3.11/dist-packages/newspaper/resources/text/"
      ],
      "metadata": {
        "id": "jtzHvgORSGYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data crawling\n",
        "* Article text will be crawled in two different ways. The first one is mild oriented to normalize the text, leave all the punctuation and will be used for summarization. The other will be aggressive cleaning with lemmatization and will be used for clusterization"
      ],
      "metadata": {
        "id": "--HKHOYsUJHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, url, stopwords):\n",
        "        self.url = url\n",
        "        self.stopwords = set(stopwords)\n",
        "        self.nlp = spacy.load(\"lt_core_news_md\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "        self.article = self.get_text(url)\n",
        "        self.raw_text = self.article.text\n",
        "\n",
        "        self.cleaned_for_summary = self.clean_text(self.raw_text, mode=\"mild\")\n",
        "        self.cleaned_for_clustering = self.clean_text(self.raw_text, mode=\"aggressive\")\n",
        "\n",
        "        main_cat, sub_cat = self.extract_categories_from_url()\n",
        "\n",
        "        self.article_data = {\n",
        "            \"URL\": self.url,\n",
        "            \"Title\": self.article.title,\n",
        "            \"Main_category\": main_cat,\n",
        "            \"Sub_category\": sub_cat,\n",
        "            \"Text\": self.raw_text,\n",
        "            \"Keywords\": ', '.join(self.article.keywords) if self.article.keywords else 'No information',\n",
        "            \"Newspaper_summary\": self.article.summary if self.article.summary else 'No summary available',\n",
        "            \"Cleaned_for_summary\": self.cleaned_for_summary,\n",
        "            \"Cleaned_for_clustering\": self.cleaned_for_clustering,\n",
        "        }\n",
        "\n",
        "    def get_text(self, url):\n",
        "        article = Article(url, language='lt')\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        article.nlp()\n",
        "        return article\n",
        "\n",
        "\"\"\"In order to extract main cand subcategories the key word was \"naujiena\". There are 3 possible variations to extract categories\n",
        "first one is when \"naujiena\" is right after 15min.lt/ in that case category will be first after that word, subcategory second.\n",
        "Second case is when \"naujiena\" is in between categorie and subcategory. The last is when there is no \"naujiena\" at all.\"\"\"\n",
        "    def extract_categories_from_url(self):\n",
        "        try:\n",
        "            parts = urlparse(self.url).path.strip(\"/\").split(\"/\")\n",
        "\n",
        "        # When /naujiena/kategorija/subkategorija\n",
        "            if parts[0] == \"naujiena\" and len(parts) > 2:\n",
        "                return parts[1].lower(), parts[2].lower()\n",
        "\n",
        "        # When /kategorija/naujiena/subkategorija\n",
        "            if \"naujiena\" in parts:\n",
        "                i = parts.index(\"naujiena\")\n",
        "                main = parts[i - 1] if i > 0 else \"Unknown\"\n",
        "                sub = parts[i + 1] if i + 1 < len(parts) else \"Unknown\"\n",
        "                return main.lower(), sub.lower()\n",
        "\n",
        "        # When /kategorija/subkategorija\n",
        "            if len(parts) >= 2:\n",
        "                return parts[0].lower(), parts[1].lower()\n",
        "\n",
        "            return \"Unknown\", \"Unknown\"\n",
        "\n",
        "        except Exception:\n",
        "            return \"Unknown\", \"Unknown\"\n",
        "\n",
        "\n",
        "    def clean_text(self, text, mode=\"mild\"):\n",
        "        # 1. Clean HTML and unwanted elements\n",
        "        clean = re.sub(r'<[^>]+>', ' ', text)            # clean HTML/XML\n",
        "        clean = re.sub(r'\\s+', ' ', clean).strip()       # align the gap sequences and trim the ends\n",
        "\n",
        "        if mode == \"mild\":\n",
        "            # Mild text cleaning - normalizing quotes and dashes\n",
        "            clean = clean.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
        "            clean = clean.replace(\"–\", \"-\")\n",
        "            return clean\n",
        "\n",
        "        elif mode == \"aggressive\":\n",
        "            # Lowering the letters, delete punctuation and numbers, lemmatizing text\n",
        "            clean = clean.lower()\n",
        "            clean = re.sub(r'[^0-9a-ąčęėįšųūĄČĘĖĮŠŲŪžŽ ]+', ' ', clean)  # leaving only letters, numbers and spaces (including specific Lithuanian laguage letters)\n",
        "            clean = re.sub(r'\\s+', ' ', clean).strip()\n",
        "            clean = re.sub(r'\\d+', ' ', clean)\n",
        "            clean = re.sub(r'\\s+', ' ', clean).strip()\n",
        "            # Lemmatizing text using spaCy\n",
        "            doc = self.nlp(clean)\n",
        "            lemmas = []\n",
        "            for token in doc:\n",
        "                lemma = token.lemma_\n",
        "                if lemma and lemma not in self.stopwords and len(lemma) > 1:\n",
        "                    lemmas.append(lemma)\n",
        "            return \" \".join(lemmas)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode for text cleaing selected: {mode}. Please select 'mild' or 'aggressive'.\")"
      ],
      "metadata": {
        "id": "P6UXU6eiSlJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"15min_straipsniu_url.csv\"\n",
        "\n",
        "def load_lt_stopwords(filepath=\"stopwords-lt.txt\"):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        stopwords = {line.strip() for line in f if line.strip()}\n",
        "    return stopwords\n",
        "\n",
        "\n",
        "def read_urls_from_csv(path):\n",
        "    df = pd.read_csv(path, header=None, names=[\"url\"])\n",
        "    unique_urls = df[\"url\"].drop_duplicates().tolist()\n",
        "\n",
        "    return unique_urls\n",
        "\n",
        "def process_articles(url_list):\n",
        "    processed_data = []\n",
        "\n",
        "    for url in url_list:\n",
        "      try:\n",
        "        tp = TextProcessor(url, stopwords=lt_stopwords)\n",
        "        processed_data.append(tp.article_data)\n",
        "      except Exception as e:\n",
        "          print(f\"There is a problem with: {url} | Error: {e}\")\n",
        "          continue\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def save_to_csv(data_list, filename=\"apdoroti_straipsniai.csv\"):\n",
        "    df = pd.DataFrame(data_list)\n",
        "    df.to_csv(filename, index=False, encoding = \"utf-8-sig\")\n",
        "    print(f\"Data was saved sucessfully to the file: {filename}\")"
      ],
      "metadata": {
        "id": "GoZpGbGrfyXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using data crawler"
      ],
      "metadata": {
        "id": "Zlho6mjug2iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lt_stopwords = load_lt_stopwords(\"stopwords-lt.txt\")\n",
        "\n",
        "# Change the name in acording to the filename\n",
        "urls = read_urls_from_csv(\"15min_straipsniu_url.csv\")\n",
        "\n",
        "article_data_list = process_articles(urls)\n",
        "\n",
        "save_to_csv(article_data_list, filename=\"apdoroti_straipsniai_1.csv\")"
      ],
      "metadata": {
        "id": "_-YxUtQXglow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data cleaning and preparation for fine-tuning\n",
        "In this part all CSVs are combined into one, dublicates and articles with less than 150 words are removed (due to summarization training)"
      ],
      "metadata": {
        "id": "oKzJGU9chKv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import modules and functions\n",
        "* glob - to find all the CSVs\n",
        "* os - to work with directories and folders"
      ],
      "metadata": {
        "id": "WOQn2tdiK2XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "3x0lzfqgMslr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preparation"
      ],
      "metadata": {
        "id": "_eqxGsYiM9qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_combine_csv_files(folder_path):\n",
        "    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
        "    dfs = [pd.read_csv(file, encoding=\"utf-8-sig\") for file in all_files]\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"Total {len(dfs)} files, article number: {len(combined_df)}.\")\n",
        "    return combined_df\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"Title\", \"Text\"], keep=\"first\")\n",
        "    after = len(df)\n",
        "    print(f\"Removed {before - after} duplicates. Total {after} unique articles left.\")\n",
        "    return df\n",
        "\n",
        "def filter_short_articles(df, min_word_count=150):\n",
        "    df[\"word_count\"] = df[\"Cleaned_for_summary\"].apply(lambda x: len(str(x).split()))\n",
        "    before = len(df)\n",
        "    df = df[df[\"word_count\"] > min_word_count]\n",
        "    after = len(df)\n",
        "    print(f\"In total {before - after} too short articles were removed. {after} articles left.\")\n",
        "    df = df.drop(columns=[\"word_count\"])\n",
        "    return df\n",
        "\n",
        "def mega_pipeline(folder_path, output_filename=\"sujungti_ir_apdoroti.csv\", min_word_count=150):\n",
        "    print(\"Cleaning data\")\n",
        "\n",
        "    df = load_and_combine_csv_files(folder_path)\n",
        "    df = remove_duplicates(df)\n",
        "    df = filter_short_articles(df, min_word_count=min_word_count)\n",
        "\n",
        "    df.to_csv(output_filename, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"Files saved: {output_filename}\")\n",
        "\n",
        "    print(\"\\nFinished\")\n",
        "\n",
        "source_folder = \"/content\"   # direction of CSVs\n",
        "output_filename = \"/content/sujungti_ir_apdoroti.csv\"  # where to saved cleanded data\n",
        "\n",
        "mega_pipeline(source_folder, output_filename)"
      ],
      "metadata": {
        "id": "5mDCy5L_NGfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Category mapping\n",
        "* due to small number or particular categories they were merged into a bigger pool"
      ],
      "metadata": {
        "id": "NvkIUHj7PQLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"sujungti_ir_apdoroti.csv\"\n",
        "df = pd.read_csv(path)\n",
        "print(f\"Total number of articles: {len(df)}\")\n",
        "\n",
        "category_mapping = {\n",
        "    '24sek': 'sportas',\n",
        "    'gazas': 'verslas',\n",
        "    'lengvai': 'gyvenimas',\n",
        "    'media-pasakojimai': 'gyvenimas',\n",
        "    'multimedija': 'gyvenimas',\n",
        "    'video': 'gyvenimas',\n",
        "    'prenumerata': 'unknown'\n",
        "}\n",
        "\n",
        "df['Main_category'] = df['Main_category'].replace(category_mapping)\n",
        "\n",
        "# Details about before and after the mapping\n",
        "\n",
        "print(\"\\n Categories before mapping:\")\n",
        "print(df['Main_category'].value_counts(dropna=False))\n",
        "\n",
        "output_path = \"sujungti_ir_apdoroti.csv\"\n",
        "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"\\nData saved: {output_path}\")\n",
        "\n",
        "print(\"\\n Categories after mapping:\")\n",
        "print(df['Main_category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "eOnPJrPNQwaN",
        "outputId": "2465d1cc-d54e-46ef-e52f-232fdfb57b51"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-1-12b579da466c>, line 14)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-12b579da466c>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    'prenumerata': 'unknown'\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT4o - mini summaries\n",
        "* for mBART fine-tuning GPT4o - mini were used to form two summaries for eatch article\n",
        "* the prompt to generate summaries was \"Pateik labai trumpą ir aiškią santrauką lietuvių kalba (1–2 sakiniai) šiam tekstui\""
      ],
      "metadata": {
        "id": "XZsxQqZMRzkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import modules\n",
        "* openai- to send requests to OpenAI to generate text\n",
        "* time - to set a pause not to overload OpenAI"
      ],
      "metadata": {
        "id": "jpoYRMshZkeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "_xDjdbAZaR2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique API key\n",
        "openai.api_key = \"sk-proj\"\n",
        "\n",
        "df = pd.read_csv('sujungti_ir_apdoroti.csv')\n",
        "\n",
        "# create columns for GPT summaries\n",
        "df['GPT4o_summary_1'] = \"\"\n",
        "df['GPT4o_summary_2'] = \"\"\n",
        "\n",
        "# GPT4o summary generation function\n",
        "def get_gpt4o_summary(text, temperature=0.7):\n",
        "    prompt = f\"Pateik labai trumpą ir aiškią santrauką lietuvių kalba (1–2 sakiniai) šiam tekstui:\\n\\n{text}\\n\\nSantrauka:\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature,\n",
        "        max_tokens=100\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"Generating summaries {index+1}/{len(df)}...\")\n",
        "\n",
        "    text = row[\"Cleaned_for_summary\"]\n",
        "\n",
        "    summary_1 = get_gpt4o_summary(text, temperature=0.7)\n",
        "    df.at[index, 'GPT4o_summary_1'] = summary_1\n",
        "    time.sleep(1)\n",
        "\n",
        "    summary_2 = get_gpt4o_summary(text, temperature=0.9)\n",
        "    df.at[index, 'GPT4o_summary_2'] = summary_2\n",
        "    time.sleep(1)\n",
        "\n",
        "df.to_csv(\"straipsniai_su_gpt4o_santraukomis.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"Summaries are generated\")"
      ],
      "metadata": {
        "id": "kFHpIkT2YnKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Until this point data is prepared and cleaned. The following part will be oriented to prepare the data for mBART fine-tuning. Out of prepared data a new dataset will be formed consisting article (mild cleaned) text and GPT4o formed summaries. Every article will be dublicated and used with eatch GPT4o summary separately"
      ],
      "metadata": {
        "id": "HtN2jCWTYnv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/straipsniai_su_gpt4o_santraukomis.csv\")\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "new_rows = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    text = row['Cleaned_for_summary']\n",
        "\n",
        "    summary1 = row['GPT4o_summary_1']\n",
        "    summary2 = row['GPT4o_summary_2']\n",
        "\n",
        "    if pd.notna(summary1) and summary1.strip():\n",
        "        new_rows.append({\"text\": text, \"summary\": summary1})\n",
        "\n",
        "    if pd.notna(summary2) and summary2.strip():\n",
        "        new_rows.append({\"text\": text, \"summary\": summary2})\n",
        "\n",
        "df_final = pd.DataFrame(new_rows)\n",
        "\n",
        "df_final.to_csv(\"/content/straipsniai_final_versija.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"Total numer of articles: {len(df_final)}. File saved as straipsniai_final_versija.csv\")\n"
      ],
      "metadata": {
        "id": "IwgNZEc8SnHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data set for mBART fine-tuning is prepared for train, validation and test (80/10/10)"
      ],
      "metadata": {
        "id": "bTLwEZjjliO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "-UEvneAol3WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/straipsniai_final_versija.csv\")\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=2025)\n",
        "\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=2025)\n",
        "\n",
        "print(f\"Train: {len(train_df)} articles\")\n",
        "print(f\"Validation: {len(val_df)} articles\")\n",
        "print(f\"Test: {len(test_df)} articles\")\n",
        "\n",
        "train_df.to_csv(\"/content/train.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "val_df.to_csv(\"/content/val.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "test_df.to_csv(\"/content/test.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Files sucessfully saved\")"
      ],
      "metadata": {
        "id": "TX66bxu8Sw8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xnId8jadl0Zw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}